{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from templates import AutoDiffFunction, Layer, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(AutoDiffFunction):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = 1/(1 + np.exp(-x))\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": y*(1-y)}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]      \n",
    "\n",
    "\n",
    "class RelU(AutoDiffFunction):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = np.where(x>0.0, 1.0, 0.0)\n",
    "\n",
    "        return x * self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        return {\"x\": self.saved_for_backward}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Layer):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.initialize_weights(in_dim, out_dim)\n",
    "\n",
    "    def initialize_weights(self, in_dim, out_dim):\n",
    "        \n",
    "        self.weights[\"w\"] = np.random.randn(in_dim, out_dim)\n",
    "        self.weights[\"b\"] = np.random.randn(1, out_dim)\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        \n",
    "        gradients = {}\n",
    "\n",
    "        # y = x * w + b        \n",
    "        # we compute gradients wrt w and x \n",
    "        # gradient wrt b is not required explicitly since we know that it's value is 1\n",
    "        gradients[\"w\"] = self.saved_for_backward[\"x\"].T\n",
    "        gradients[\"x\"] = self.weights[\"w\"].T\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = x @ self.weights[\"w\"] + self.weights[\"b\"]\n",
    "        self.saved_for_backward[\"x\"] = x\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, dy):\n",
    "        #print(dy.shape)\n",
    "        #print(self.grad[\"x\"].shape)\n",
    "        #print(self.grad[\"w\"].shape)\n",
    "        \n",
    "        dx = dy @ self.grad[\"x\"]\n",
    "        \n",
    "        # calculating gradients wrt weights\n",
    "        dw = self.grad[\"w\"] @ dy\n",
    "        db = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "        self.absolute_gradients = {\"w\": dw, \"b\": db}\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.optimizer.step(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the loss function\n",
    "\n",
    "For this particular problem, we require CrossEntropy Loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0618754 -0.        -0.       ]\n",
      " [-0.         0.4462871 -0.       ]]\n",
      "[0.0618754 0.4462871]\n",
      "0.2540812531732535\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.94, 0.04, 0.02],[0.3, 0.64, 0.06]])\n",
    "\n",
    "y = np.array([[1,0,0],[0,1,0]])\n",
    "\n",
    "z = -y * np.log(x)\n",
    "print(z)\n",
    "z = np.sum(z, axis=1)\n",
    "print(z)\n",
    "print(np.mean(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFromLogits(Loss):\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        v = np.exp(x)\n",
    "\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(y): \n",
    "        d = len(np.unique(y))\n",
    "        encoded_y = np.zeros(shape=(len(y), d))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "\n",
    "        return encoded_y\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "         \n",
    "        probabilities = self.softmax(y_pred)\n",
    "        y_true_encoded = self.encode(y_true)\n",
    "\n",
    "        loss_value = np.mean(np.sum(- y_true_encoded * np.log(probabilities), axis=1))\n",
    "\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def compute_grad(self, y_pred, y_true):\n",
    "\n",
    "        return {\"x\": self.saved_for_backward[\"probabilities\"] - self.saved_for_backward[\"y_true\"]}        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an optimizer for the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, layer):\n",
    "        for weight_name, _ in layer.weights.items():\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - self.lr * layer.absolute_gradients[weight_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the structure for an actual neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.forward(*args, **kwds)\n",
    "\n",
    "    def compile(self, loss, optimizer):\n",
    "        self.loss = loss\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.optimizer = deepcopy(optimizer)\n",
    "\n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        return self.loss(y_pred, y_true)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        gradient = self.loss.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def update_weights(self):\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom classification dataset to test out the function <br> dsdsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a dummy dataset to test out stuff ##\n",
    "\n",
    "X, y = make_classification(n_samples=32*6, n_features=20, n_informative=15, n_classes=3)\n",
    "\n",
    "def create_batches(X, y, batch_size=32):\n",
    "    batches = []\n",
    "\n",
    "    for i in range(len(y) // batch_size):\n",
    "        start_idx = batch_size * i\n",
    "        end_idx = batch_size * (i + 1)\n",
    "\n",
    "        batches.append([X[start_idx: end_idx], y[start_idx: end_idx]])\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Utility functions ##\n",
    "def probs_to_labels(y): \n",
    "    return np.argmax(y, axis=1)\n",
    "\n",
    "\n",
    "def encoded_to_labels(y):\n",
    "    return np.where(y==1)[1]\n",
    "\n",
    "def accuracy_score(y_pred, y_true):\n",
    "\n",
    "    pred_labels = probs_to_labels(y_pred)\n",
    "\n",
    "    return np.sum(pred_labels == y_true) / len(y_true)\n",
    "\n",
    "batches = create_batches(X, y, batch_size=32)\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet([FC(20, 32), RelU(), FC(32, 3)])\n",
    "optimizer = SGD(lr = 0.001)\n",
    "\n",
    "def fit_model(model, batches, optimizer, epochs=10):\n",
    "\n",
    "    training_stats = []\n",
    "    num_batches = len(batches)\n",
    " \n",
    "    loss = CrossEntropyLossFromLogits()\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        for X, y in batches:\n",
    "\n",
    "            preds = model(X)\n",
    "            total_loss += model.loss(preds, y)\n",
    "            total_accuracy += accuracy_score(preds, y)\n",
    "\n",
    "            _ = model.backward()\n",
    "            model.update_weights()\n",
    "\n",
    "        loss_per_epoch = total_loss / num_batches\n",
    "        accuracy = total_accuracy / num_batches\n",
    "\n",
    "        print(f\"Epoch: {epoch} Train Loss: {loss_per_epoch} Train Accuracy: {accuracy}\")\n",
    "\n",
    "        training_stats.append({\"Epoch\" : epoch, \n",
    "                                \"Train Loss\": loss_per_epoch,\n",
    "                                \"Train Accuracy\": accuracy})\n",
    "\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.030437021236744535 Train Accuracy: 0.9947916666666666\n",
      "Epoch: 2 Train Loss: 0.02824352679668765 Train Accuracy: 1.0\n",
      "Epoch: 3 Train Loss: 0.026414579685934764 Train Accuracy: 1.0\n",
      "Epoch: 4 Train Loss: 0.024833449023869206 Train Accuracy: 1.0\n",
      "Epoch: 5 Train Loss: 0.023397073284273224 Train Accuracy: 1.0\n",
      "Epoch: 6 Train Loss: 0.02217035885632808 Train Accuracy: 1.0\n",
      "Epoch: 7 Train Loss: 0.021088792700828075 Train Accuracy: 1.0\n",
      "Epoch: 8 Train Loss: 0.020129594902831185 Train Accuracy: 1.0\n",
      "Epoch: 9 Train Loss: 0.019233363186824337 Train Accuracy: 1.0\n",
      "Epoch: 10 Train Loss: 0.018433657410856025 Train Accuracy: 1.0\n",
      "Epoch: 11 Train Loss: 0.01770867252126077 Train Accuracy: 1.0\n",
      "Epoch: 12 Train Loss: 0.01704596157986977 Train Accuracy: 1.0\n",
      "Epoch: 13 Train Loss: 0.01643315172463195 Train Accuracy: 1.0\n",
      "Epoch: 14 Train Loss: 0.015897271966827633 Train Accuracy: 1.0\n",
      "Epoch: 15 Train Loss: 0.015357063895110983 Train Accuracy: 1.0\n",
      "Epoch: 16 Train Loss: 0.014869907805205014 Train Accuracy: 1.0\n",
      "Epoch: 17 Train Loss: 0.01442719352179211 Train Accuracy: 1.0\n",
      "Epoch: 18 Train Loss: 0.014012404319194252 Train Accuracy: 1.0\n",
      "Epoch: 19 Train Loss: 0.013616271860350242 Train Accuracy: 1.0\n",
      "Epoch: 20 Train Loss: 0.013249982018234262 Train Accuracy: 1.0\n",
      "Epoch: 21 Train Loss: 0.012914592055096529 Train Accuracy: 1.0\n",
      "Epoch: 22 Train Loss: 0.012586284983418615 Train Accuracy: 1.0\n",
      "Epoch: 23 Train Loss: 0.012269806459370175 Train Accuracy: 1.0\n",
      "Epoch: 24 Train Loss: 0.01198097025823479 Train Accuracy: 1.0\n",
      "Epoch: 25 Train Loss: 0.011698421871812975 Train Accuracy: 1.0\n",
      "Epoch: 26 Train Loss: 0.011433011178672112 Train Accuracy: 1.0\n",
      "Epoch: 27 Train Loss: 0.011188455931070729 Train Accuracy: 1.0\n",
      "Epoch: 28 Train Loss: 0.010936725507155742 Train Accuracy: 1.0\n",
      "Epoch: 29 Train Loss: 0.010716198713718162 Train Accuracy: 1.0\n",
      "Epoch: 30 Train Loss: 0.010491865203304652 Train Accuracy: 1.0\n",
      "Epoch: 31 Train Loss: 0.010284982528588706 Train Accuracy: 1.0\n",
      "Epoch: 32 Train Loss: 0.010083285654244007 Train Accuracy: 1.0\n",
      "Epoch: 33 Train Loss: 0.009897556735900167 Train Accuracy: 1.0\n",
      "Epoch: 34 Train Loss: 0.009709531038341036 Train Accuracy: 1.0\n",
      "Epoch: 35 Train Loss: 0.009532482004038145 Train Accuracy: 1.0\n",
      "Epoch: 36 Train Loss: 0.009362176271898474 Train Accuracy: 1.0\n",
      "Epoch: 37 Train Loss: 0.009198228101154888 Train Accuracy: 1.0\n",
      "Epoch: 38 Train Loss: 0.00904306080552033 Train Accuracy: 1.0\n",
      "Epoch: 39 Train Loss: 0.00889637130266431 Train Accuracy: 1.0\n",
      "Epoch: 40 Train Loss: 0.008748215563969 Train Accuracy: 1.0\n",
      "Epoch: 41 Train Loss: 0.008605478163625576 Train Accuracy: 1.0\n",
      "Epoch: 42 Train Loss: 0.008471056925346429 Train Accuracy: 1.0\n",
      "Epoch: 43 Train Loss: 0.008341018069519706 Train Accuracy: 1.0\n",
      "Epoch: 44 Train Loss: 0.008213030893242426 Train Accuracy: 1.0\n",
      "Epoch: 45 Train Loss: 0.008089682445601784 Train Accuracy: 1.0\n",
      "Epoch: 46 Train Loss: 0.007977036813740594 Train Accuracy: 1.0\n",
      "Epoch: 47 Train Loss: 0.007860262953088974 Train Accuracy: 1.0\n",
      "Epoch: 48 Train Loss: 0.007746258085351885 Train Accuracy: 1.0\n",
      "Epoch: 49 Train Loss: 0.007639511493398754 Train Accuracy: 1.0\n",
      "Epoch: 50 Train Loss: 0.0075326522037302635 Train Accuracy: 1.0\n",
      "Epoch: 51 Train Loss: 0.0074338429241689935 Train Accuracy: 1.0\n",
      "Epoch: 52 Train Loss: 0.007333314089684466 Train Accuracy: 1.0\n",
      "Epoch: 53 Train Loss: 0.007240031537332655 Train Accuracy: 1.0\n",
      "Epoch: 54 Train Loss: 0.007144827583372635 Train Accuracy: 1.0\n",
      "Epoch: 55 Train Loss: 0.007054700908407208 Train Accuracy: 1.0\n",
      "Epoch: 56 Train Loss: 0.006964676171419606 Train Accuracy: 1.0\n",
      "Epoch: 57 Train Loss: 0.006879223687507762 Train Accuracy: 1.0\n",
      "Epoch: 58 Train Loss: 0.006795252257644956 Train Accuracy: 1.0\n",
      "Epoch: 59 Train Loss: 0.006714587252682418 Train Accuracy: 1.0\n",
      "Epoch: 60 Train Loss: 0.006636629509157304 Train Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "training_stats = fit_model(model, batches, SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}