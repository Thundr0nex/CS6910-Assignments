{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px; text-align:center\">Assignment 1</p>\n",
    "<p style=\"font-size:18px; text-align:center\">CS6910: Fundamentals of Deep Learning</p>\n",
    "<p>Sujay Bokil: ME17B120<br>\n",
    "Avyay Rao: ME17B130</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from copy import deepcopy\n",
    "import numpy as np \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# import templates that we have created to make different kinds of layers, losses, optimizers etc.\n",
    "from sujay.templates import AutoDiffFunction, Layer, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Outline of the framework</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Activation functions</b> \n",
    "\n",
    "For this assignment we, write 2 activation functions \n",
    "\n",
    "1. Sigmoid activation\n",
    "\n",
    "$$y = \\sigma(x) = \\frac{1}{1 + e^x}$$\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{-e^x}{(1 + e^x)^2} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "2. ReLU activation\n",
    "\n",
    "$$y = ReLU(x) = max(0, x)$$\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x\\geq 0\\\\\n",
    "      0 & x\\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(AutoDiffFunction):\n",
    "    \"\"\" Represents the Sigmoid Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = 1/(1 + np.exp(-x))\n",
    "        return self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        y = self.saved_for_backward\n",
    "\n",
    "        return {\"x\": y*(1-y)}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]      \n",
    "\n",
    "\n",
    "class ReLU(AutoDiffFunction):\n",
    "    \"\"\" Represents the RelU Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.saved_for_backward = np.where(x>0.0, 1.0, 0.0)\n",
    "\n",
    "        return x * self.saved_for_backward\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        return {\"x\": self.saved_for_backward}\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy * self.grad[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Layers</b>\n",
    "\n",
    "For this assignment, we only fully connected OR Dense layers where each input neuron is connected to each output neuron, along with a bias unit. Below is a representation of a fully connected layer.\n",
    "\n",
    "![Representation of a fully connected layer](FullyConnectedLayer.png)\n",
    "\n",
    "The equation for such a layer is simply\n",
    "\n",
    "$$y = FullyConnected(x) = wx + b$$\n",
    "\n",
    "$$\\frac{dy}{dw} = x^T \\quad\\frac{dy}{dx} = w^T  \\quad\\frac{dy}{db} = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(Layer):\n",
    "    \"\"\"Class representing a fully connected layer, the weights inside the class decide it's output\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.initialize_weights(in_dim, out_dim)\n",
    "\n",
    "    def initialize_weights(self, in_dim, out_dim):\n",
    "        \n",
    "        self.weights[\"w\"] = np.random.randn(in_dim, out_dim)\n",
    "        self.weights[\"b\"] = np.random.randn(1, out_dim)\n",
    "\n",
    "    def compute_grad(self, x):\n",
    "        \n",
    "        gradients = {}\n",
    "\n",
    "        # y = x * w + b        \n",
    "        # we compute gradients wrt w and x \n",
    "        # gradient wrt b is not required explicitly since we know that it's value is 1\n",
    "        gradients[\"w\"] = self.saved_for_backward[\"x\"].T\n",
    "        gradients[\"x\"] = self.weights[\"w\"].T\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = x @ self.weights[\"w\"] + self.weights[\"b\"]\n",
    "        self.saved_for_backward[\"x\"] = x\n",
    "        self.saved_for_backward[\"w\"] = w\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, dy):\n",
    "        dx = dy @ self.grad[\"x\"]\n",
    "        \n",
    "        # calculating gradients wrt weights\n",
    "        dw = self.grad[\"w\"] @ dy\n",
    "        db = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "        self.absolute_gradients = {\"w\": dw, \"b\": db}\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.optimizer.step(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loss function</b>\n",
    "\n",
    "The loss function dictates how good the output of the neural network is. Since we use MNIST dataset, our job is classification and hence we use the Categorical CrossEntropy loss function. The equation is given as\n",
    "\n",
    "\n",
    "$$L(p, y) = \\Sigma_{i=1}^{N} \\Sigma_{k=1}^{K} y_{ik} \\log p_{ik}$$ \n",
    "\n",
    "where $$y_{ik} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x \\in class-k\\\\\n",
    "      0 & else \\\\\n",
    "\\end{array} \n",
    "\\right.$$\n",
    "\n",
    "$p_{ik} =$ probability that $i^{th}$ sample falls in $k^{th}$ class\n",
    "\n",
    "In our implementation, the given loss function is applied along with the activation function for the last layer i.e. Softmax activation. It's formula is given by the following equation\n",
    "\n",
    "$ f: [x_1, x_2, ... x_k] \\rightarrow [p_1, p_2, ... p_k]$ such that $p_i = \\frac{e^{x_i}}{\\Sigma_{k=1}^{K} e^{x_i}}$\n",
    "\n",
    "Now, to find the derivative of loss w.r.t input we have apply the chain rule. Let $p(x)$ represent the softmax activation and $L$ represent the loss. Then the expression turns out to be\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L(p, y)}{\\partial p} \\frac{\\partial p(x)}{\\partial x} = p - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossFromLogits(Loss):\n",
    "    \"\"\" Class that holds CrossEntropy loss along with softmax activation\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        v = np.exp(x)\n",
    "\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(y): \n",
    "        d = len(np.unique(y))\n",
    "        encoded_y = np.zeros(shape=(len(y), d))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "\n",
    "        return encoded_y\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "         \n",
    "        probabilities = self.softmax(y_pred)\n",
    "        y_true_encoded = self.encode(y_true)\n",
    "\n",
    "        loss_value = np.mean(np.sum(- y_true_encoded * np.log(probabilities), axis=1))\n",
    "\n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def compute_grad(self, y_pred, y_true):\n",
    "\n",
    "        return {\"x\": self.saved_for_backward[\"probabilities\"] - self.saved_for_backward[\"y_true\"]} \n",
    "    \n",
    "\n",
    "class MSE(Loss):\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        v = np.exp(x)\n",
    "\n",
    "        return v / np.sum(v, axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(y): \n",
    "        d = len(np.unique(y))\n",
    "        encoded_y = np.zeros(shape=(len(y), d))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            encoded_y[i,y[i]] = 1\n",
    "\n",
    "        return encoded_y\n",
    "    \n",
    "    def forward(y_pred, y_true):\n",
    "        \n",
    "        probabilities = self.softmax(y_pred)\n",
    "        y_true_encoded = self.encode(y_true)\n",
    "        \n",
    "        loss_value = np.mean(np.sum((probabilities - y_true_encoded)**2, axis=1))\n",
    "        \n",
    "        self.saved_for_backward[\"probabilities\"] = probabilities\n",
    "        self.saved_for_backward[\"y_true\"] = y_true_encoded\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    def compute_grad(y_pred, y_true):\n",
    "        return {\"x\": 2 * (self.saved_for_backward[\"probabilities\"] - self.saved_for_backward[\"y_true\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optimizers</b>\n",
    "\n",
    "For this assignment, we have created the 6 optimizers given in the question\n",
    "\n",
    "1. sgd\n",
    "2. momentum based gradient descent\n",
    "3. nesterov accelerated gradient descent\n",
    "4. rmsprop\n",
    "5. adam\n",
    "6. nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, layer):\n",
    "        for weight_name, _ in layer.weights.items():\n",
    "            layer.weights[weight_name] = layer.weights[weight_name] - self.lr * layer.absolute_gradients[weight_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Structuring the neural network</b>\n",
    "\n",
    "We create a class to hold all the above components together, so that we can initialize a custom neural network with required layer sizes, activations and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\r\n",
    "    def __init__(self, layers) -> None:\r\n",
    "        self.layers = layers\r\n",
    "\r\n",
    "    def __call__(self, *args, **kwds):\r\n",
    "        return self.forward(*args, **kwds)\r\n",
    "\r\n",
    "    def compile(self, loss, optimizer):\r\n",
    "        self.loss = loss\r\n",
    "\r\n",
    "        for layer in self.layers:\r\n",
    "            if isinstance(layer, Layer):\r\n",
    "                layer.optimizer = deepcopy(optimizer)\r\n",
    "\r\n",
    "    def calculate_loss(self, y_pred, y_true):\r\n",
    "        return self.loss(y_pred, y_true)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        for layer in self.layers:\r\n",
    "            x = layer(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "    def backward(self):\r\n",
    "\r\n",
    "        gradient = self.loss.backward()\r\n",
    "        for layer in reversed(self.layers):\r\n",
    "            gradient = layer.backward(gradient)\r\n",
    "\r\n",
    "        return gradient\r\n",
    "\r\n",
    "    def update_weights(self):\r\n",
    "\r\n",
    "        for layer in reversed(self.layers):\r\n",
    "            if isinstance(layer, Layer):\r\n",
    "                layer.update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Utility functions</b>\n",
    "\n",
    "We add some utility functions to preprocess and batch the dataset given, and fit the model to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(X, y, batch_size=32):\n",
    "    \"\"\"Creates batches of the dataset\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "\n",
    "    for i in range(len(y) // batch_size):\n",
    "        start_idx = batch_size * i\n",
    "        end_idx = batch_size * (i + 1)\n",
    "\n",
    "        batches.append([X[start_idx: end_idx], y[start_idx: end_idx]])\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def fit_model(model, batches, optimizer, epochs=10):\n",
    "    \"\"\" Trains the model on the given data\n",
    "    \"\"\"\n",
    "    training_stats = []\n",
    "    num_batches = len(batches)\n",
    " \n",
    "    loss = CrossEntropyLossFromLogits()\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        for X, y in batches:\n",
    "\n",
    "            preds = model(X)\n",
    "            total_loss += model.loss(preds, y)\n",
    "            total_accuracy += accuracy_score(preds, y)\n",
    "\n",
    "            _ = model.backward()\n",
    "            model.update_weights()\n",
    "\n",
    "        loss_per_epoch = total_loss / num_batches\n",
    "        accuracy = total_accuracy / num_batches\n",
    "\n",
    "        print(f\"Epoch: {epoch} Train Loss: {loss_per_epoch} Train Accuracy: {accuracy}\")\n",
    "\n",
    "        training_stats.append({\"Epoch\" : epoch, \n",
    "                                \"Train Loss\": loss_per_epoch,\n",
    "                                \"Train Accuracy\": accuracy})\n",
    "\n",
    "    \n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Downloading the Fashion MNIST dataset using kers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}